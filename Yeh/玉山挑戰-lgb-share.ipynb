{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder,StandardScaler\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "pd.options.mode.chained_assignment = None\n",
    "import warnings\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split,learning_curve,cross_val_score,KFold,TimeSeriesSplit,GridSearchCV\n",
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import (GradientBoostingClassifier, GradientBoostingRegressor, \n",
    "                              RandomForestClassifier, RandomForestRegressor,AdaBoostClassifier) \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "import lightgbm as lgbm\n",
    "from sklearn.metrics import f1_score, confusion_matrix,roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('submission_test.csv')\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train.append(test)\n",
    "data.reset_index(inplace=True, drop=True) #index重複\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#補空值\n",
    "\n",
    "# data['flg_3dsmk'] = data[['ecfg','flg_3dsmk']].apply(lambda x: x['ecfg'] if x['ecfg']==0 else x['flg_3dsmk'], axis=1)\n",
    "# # data['flg_3dsmk'] = data[['stocn','flg_3dsmk']].apply(lambda x: 0 if x['stocn']==46 else x['flg_3dsmk'], axis=1)\n",
    "# # data['flg_3dsmk'] = data[['stocn','flg_3dsmk']].apply(lambda x: 0 if x['stocn']==49 else x['flg_3dsmk'], axis=1)\n",
    "# # data['flg_3dsmk'] = data[['stocn','flg_3dsmk']].apply(lambda x: 0 if x['stocn']==47 else x['flg_3dsmk'], axis=1)\n",
    "# # data['flg_3dsmk'] = data[['mcc','flg_3dsmk']].apply(lambda x: 0 if x['mcc']==5 else x['flg_3dsmk'], axis=1)\n",
    "# # data['flg_3dsmk'] = data[['mcc','flg_3dsmk']].apply(lambda x: 0 if x['mcc']==31 else x['flg_3dsmk'], axis=1)\n",
    "# # data['flg_3dsmk'] = data[['mcc','flg_3dsmk']].apply(lambda x: 0 if x['mcc']==53 else x['flg_3dsmk'], axis=1)\n",
    "# # data['flg_3dsmk'] = data[['mcc','flg_3dsmk']].apply(lambda x: 0 if x['mcc']==205 else x['flg_3dsmk'], axis=1)\n",
    "# # data['flg_3dsmk'] = data[['mcc','flg_3dsmk']].apply(lambda x: 0 if x['mcc']==221 else x['flg_3dsmk'], axis=1)\n",
    "# # data['flg_3dsmk'] = data[['mcc','flg_3dsmk']].apply(lambda x: 0 if x['mcc']==226 else x['flg_3dsmk'], axis=1)\n",
    "# # data['flg_3dsmk'] = data[['mcc','flg_3dsmk']].apply(lambda x: 0 if x['mcc']==356 else x['flg_3dsmk'], axis=1)\n",
    "# # data['flg_3dsmk'] = data[['mcc','flg_3dsmk']].apply(lambda x: 0 if x['mcc']==361 else x['flg_3dsmk'], axis=1)\n",
    "# # data['flg_3dsmk'] = data[['mcc','flg_3dsmk']].apply(lambda x: 0 if x['mcc']==372 else x['flg_3dsmk'], axis=1)\n",
    "# # data['flg_3dsmk'] = data[['mcc','flg_3dsmk']].apply(lambda x: 0 if x['mcc']==407 else x['flg_3dsmk'], axis=1)\n",
    "# # data['flg_3dsmk'] = data[['mcc','flg_3dsmk']].apply(lambda x: 0 if x['mcc']==421 else x['flg_3dsmk'], axis=1)\n",
    "\n",
    "\n",
    "# data['flbmk'] = data[['ecfg','flbmk']].apply(lambda x: 0 if x['ecfg']==1 else x['flbmk'], axis=1)\n",
    "# data['flbmk'] = data[['contp','flbmk']].apply(lambda x: 0 if x['contp']==2 else x['flbmk'], axis=1)\n",
    "# data['flbmk'] = data[['contp','flbmk']].apply(lambda x: 0 if x['contp']==4 else x['flbmk'], axis=1)\n",
    "# data['flbmk'] = data[['contp','flbmk']].apply(lambda x: 0 if x['contp']==6 else x['flbmk'], axis=1)\n",
    "# data['flbmk'] = data[['etymd','flbmk']].apply(lambda x: 0 if x['etymd']==0 else x['flbmk'], axis=1)\n",
    "# data['flbmk'] = data[['etymd','flbmk']].apply(lambda x: 0 if x['etymd']==1 else x['flbmk'], axis=1)\n",
    "# data['flbmk'] = data[['etymd','flbmk']].apply(lambda x: 0 if x['etymd']==2 else x['flbmk'], axis=1)\n",
    "# data['flbmk'] = data[['etymd','flbmk']].apply(lambda x: 0 if x['etymd']==4 else x['flbmk'], axis=1)\n",
    "# data['flbmk'] = data[['etymd','flbmk']].apply(lambda x: 0 if x['etymd']==5 else x['flbmk'], axis=1)\n",
    "# data['flbmk'] = data[['etymd','flbmk']].apply(lambda x: 0 if x['etymd']==6 else x['flbmk'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['ecfg'] = data['ecfg'].astype('category').cat.codes\n",
    "data['flbmk'] = data['flbmk'].astype('category').cat.codes\n",
    "data['flg_3dsmk'] = data['flg_3dsmk'].astype('category').cat.codes\n",
    "data['insfg'] = data['insfg'].astype('category').cat.codes\n",
    "data['ovrlt'] = data['ovrlt'].astype('category').cat.codes\n",
    "# #數值\n",
    "data['acqic'] = data['acqic'].astype('category').cat.codes\n",
    "data['bacno'] = data['bacno'].astype('category').cat.codes\n",
    "data['cano'] = data['cano'].astype('category').cat.codes\n",
    "data['csmcu'] = data['csmcu'].astype('category').cat.codes\n",
    "data['hcefg'] = data['hcefg'].astype('category').cat.codes\n",
    "data['mcc'] = data['mcc'].astype('category').cat.codes\n",
    "data['mchno'] = data['mchno'].astype('category').cat.codes\n",
    "data['scity'] = data['scity'].astype('category').cat.codes\n",
    "data['stocn'] = data['stocn'].astype('category').cat.codes\n",
    "\n",
    "\n",
    "# data['contp'] = data['contp'].astype('int32')\n",
    "# data['etymd'] = data['etymd'].astype('int32')\n",
    "# data['iterm'] = data['iterm'].astype('int32')\n",
    "# data['locdt'] = data['locdt'].astype('int32')\n",
    "# data['loctm'] = data['loctm'].astype('int32')\n",
    "# data['stscd'] = data['stscd'].astype('int32')\n",
    "# train['fraud_ind'] = train['fraud_ind'].astype('int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['loctm_int'] = data['loctm'].astype(int)\n",
    "data['loctm_str'] = data['loctm_int'].astype(str)\n",
    "data['loctm_str'] = data['loctm_str'].apply(lambda x : '0'+x if (len(x)==5) else ('00'+x if (len(x)==4) else ('000'+x if (len(x)==3) else ('0000'+x if (len(x)==2) else ('00000'+x if (len(x)==1) else x)))))\n",
    "data['hours'] = data['loctm_str'].str[0:2]\n",
    "data['minutes'] = data['loctm_str'].str[2:4]\n",
    "data['seconds'] = data['loctm_str'].str[4:6]\n",
    "data['hours'] = data['hours'].astype(int)\n",
    "data['minutes'] = data['minutes'].astype(int)\n",
    "data['seconds'] = data['seconds'].astype(int)\n",
    "data['total_seconds'] = data['locdt']*86400+data['hours']*3600+data['minutes']*60+data['seconds']\n",
    "data['time'] = (data['hours']*3600+data['minutes']*60+data['seconds'])#/86400\n",
    "# data['timeBin'] = pd.cut(data['time'], 24)\n",
    "# data['timeBin'] = data['timeBin'].astype('category').cat.codes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['decimal'] = data['conam'] - data['conam'].astype(int)\n",
    "data['taiwan_ornot'] = data['stocn'].apply(lambda x : 1 if x==102 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[pd.notnull(data['fraud_ind'])]\n",
    "test = data[~pd.notnull(data['fraud_ind'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['date_day'] = train['locdt'] % 30\n",
    "test['date_day'] = test['locdt'] % 30\n",
    "\n",
    "train['week'] = train['locdt'] % 7\n",
    "test['week'] = test['locdt'] % 7\n",
    "\n",
    "train['2_week'] = train['locdt'] % 14\n",
    "test['2_week'] = test['locdt'] % 14\n",
    "\n",
    "train['month'] = train['locdt'] % 30\n",
    "test['month'] = test['locdt'] % 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transation_count = train.groupby(['bacno'])['cano'].count().rename(\"transation_count\").reset_index()\n",
    "train = train.merge(transation_count)\n",
    "transation_count = test.groupby(['bacno'])['cano'].count().rename(\"transation_count\").reset_index()\n",
    "test = test.merge(transation_count)\n",
    "\n",
    "transation_count_cano = train.groupby(['bacno','cano'])['cano'].count().rename(\"transation_count_cano\").reset_index()\n",
    "train = train.merge(transation_count_cano)\n",
    "transation_count_cano = test.groupby(['bacno','cano'])['cano'].count().rename(\"transation_count_cano\").reset_index()\n",
    "test = test.merge(transation_count_cano)\n",
    "\n",
    "acqic_duplicated_count = train.groupby(['bacno','acqic'])['acqic'].count().rename(\"acqic_duplicated_count\").reset_index()\n",
    "train = train.merge(acqic_duplicated_count)\n",
    "acqic_duplicated_count = test.groupby(['bacno','acqic'])['acqic'].count().rename(\"acqic_duplicated_count\").reset_index()\n",
    "test = test.merge(acqic_duplicated_count)\n",
    "\n",
    "conam_duplicated_count = train.groupby(['bacno','conam'])['conam'].count().rename(\"conam_duplicated_count\").reset_index()\n",
    "train = train.merge(conam_duplicated_count)\n",
    "conam_duplicated_count = test.groupby(['bacno','conam'])['conam'].count().rename(\"conam_duplicated_count\").reset_index()\n",
    "test = test.merge(conam_duplicated_count)\n",
    "\n",
    "conam_stocn = train.groupby(['csmcu','stocn'])['stocn'].count().rename(\"conam_stocn\").reset_index()\n",
    "train = train.merge(conam_stocn, on=['csmcu', 'stocn'])\n",
    "conam_stocn = test.groupby(['csmcu','stocn'])['stocn'].count().rename(\"conam_stocn\").reset_index()\n",
    "test = test.merge(conam_stocn, on=['csmcu', 'stocn'])\n",
    "\n",
    "bacno_mchno = train.groupby(['bacno','mchno'])['mchno'].count().rename(\"bacno_mchno\").reset_index()\n",
    "train = train.merge(bacno_mchno, on=['bacno', 'mchno'])\n",
    "bacno_mchno = test.groupby(['bacno','mchno'])['mchno'].count().rename(\"bacno_mchno\").reset_index()\n",
    "test = test.merge(bacno_mchno, on=['bacno', 'mchno'])\n",
    "\n",
    "cano_mchno = train.groupby(['cano','mchno'])['mchno'].count().rename(\"cano_mchno\").reset_index()\n",
    "train = train.merge(cano_mchno, on=['cano', 'mchno'])\n",
    "cano_mchno = test.groupby(['cano','mchno'])['mchno'].count().rename(\"cano_mchno\").reset_index()\n",
    "test = test.merge(cano_mchno, on=['cano', 'mchno'])\n",
    "\n",
    "bacno_stocn = train.groupby(['bacno','stocn'])['stocn'].count().rename(\"bacno_stocn\").reset_index()\n",
    "train = train.merge(bacno_stocn, on=['bacno', 'stocn'])\n",
    "bacno_stocn = test.groupby(['bacno','stocn'])['stocn'].count().rename(\"bacno_stocn\").reset_index()\n",
    "test = test.merge(bacno_stocn, on=['bacno', 'stocn'])\n",
    "\n",
    "cano_stocn = train.groupby(['cano','stocn'])['stocn'].count().rename(\"cano_stocn\").reset_index()\n",
    "train = train.merge(cano_stocn, on=['cano', 'stocn'])\n",
    "cano_stocn = test.groupby(['cano','stocn'])['stocn'].count().rename(\"cano_stocn\").reset_index()\n",
    "test = test.merge(cano_stocn, on=['cano', 'stocn'])\n",
    "\n",
    "bacno_scity = train.groupby(['bacno','scity'])['scity'].count().rename(\"bacno_scity\").reset_index()\n",
    "train = train.merge(bacno_scity, on=['bacno', 'scity'])\n",
    "bacno_scity = test.groupby(['bacno','scity'])['scity'].count().rename(\"bacno_scity\").reset_index()\n",
    "test = test.merge(bacno_scity, on=['bacno', 'scity'])\n",
    "\n",
    "cano_scity = train.groupby(['cano','scity'])['scity'].count().rename(\"cano_scity\").reset_index()\n",
    "train = train.merge(cano_scity, on=['cano', 'scity'])\n",
    "cano_scity = test.groupby(['cano','scity'])['scity'].count().rename(\"cano_scity\").reset_index()\n",
    "test = test.merge(cano_scity, on=['cano', 'scity'])\n",
    "\n",
    "\n",
    "bacno_flg_3dsmk = train.groupby(['bacno','flg_3dsmk'])['flg_3dsmk'].count().rename(\"bacno_flg_3dsmk\").reset_index()\n",
    "train = train.merge(bacno_flg_3dsmk, on=['bacno', 'flg_3dsmk'])\n",
    "bacno_flg_3dsmk = test.groupby(['bacno','flg_3dsmk'])['flg_3dsmk'].count().rename(\"bacno_flg_3dsmk\").reset_index()\n",
    "test = test.merge(bacno_flg_3dsmk, on=['bacno', 'flg_3dsmk'])\n",
    "\n",
    "cano_flg_3dsmk = train.groupby(['cano','flg_3dsmk'])['flg_3dsmk'].count().rename(\"cano_flg_3dsmk\").reset_index()\n",
    "train = train.merge(cano_flg_3dsmk, on=['cano', 'flg_3dsmk'])\n",
    "cano_flg_3dsmk = test.groupby(['cano','flg_3dsmk'])['flg_3dsmk'].count().rename(\"cano_flg_3dsmk\").reset_index()\n",
    "test = test.merge(cano_flg_3dsmk, on=['cano', 'flg_3dsmk'])\n",
    "\n",
    "\n",
    "bacno_ecfg_mean = train.groupby(['bacno'])['ecfg'].mean().rename(\"bacno_ecfg_mean\").reset_index()\n",
    "train = train.merge(bacno_ecfg_mean, on=['bacno'])\n",
    "bacno_ecfg_mean = test.groupby(['bacno'])['ecfg'].mean().rename(\"bacno_ecfg_mean\").reset_index()\n",
    "test = test.merge(bacno_ecfg_mean, on=['bacno'])\n",
    "\n",
    "cano_ecfg_mean = train.groupby(['cano'])['ecfg'].mean().rename(\"cano_ecfg_mean\").reset_index()\n",
    "train = train.merge(cano_ecfg_mean, on=['cano'])\n",
    "cano_ecfg_mean = test.groupby(['cano'])['ecfg'].mean().rename(\"cano_ecfg_mean\").reset_index()\n",
    "test = test.merge(cano_ecfg_mean, on=['cano'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_amount = train.groupby(['bacno'])['conam'].mean().rename(\"mean_amount\").reset_index()\n",
    "train = train.merge(mean_amount)\n",
    "mean_amount = test.groupby(['bacno'])['conam'].mean().rename(\"mean_amount\").reset_index()\n",
    "test = test.merge(mean_amount)\n",
    "\n",
    "train['amtby_mean_amount'] = train['conam'] / train['mean_amount']\n",
    "train['amtby_mean_amount'] = train['amtby_mean_amount'].fillna(0)\n",
    "test['amtby_mean_amount'] = test['conam'] / test['mean_amount']\n",
    "test['amtby_mean_amount'] = test['amtby_mean_amount'].fillna(0)\n",
    "\n",
    "\n",
    "# train['conam_mean_diff'] = train['conam'] - train['mean_amount']\n",
    "# test['conam_mean_diff'] = test['conam'] - train['mean_amount']\n",
    "\n",
    "mean_amount_cano = train.groupby(['bacno','cano'])['conam'].mean().rename(\"mean_amount_cano\").reset_index()\n",
    "train = train.merge(mean_amount_cano)\n",
    "mean_amount_cano = test.groupby(['bacno','cano'])['conam'].mean().rename(\"mean_amount_cano\").reset_index()\n",
    "test = test.merge(mean_amount_cano)\n",
    "\n",
    "train['amtby_mean_amount_cano'] = train['conam'] / train['mean_amount_cano']\n",
    "train['amtby_mean_amount_cano'] = train['amtby_mean_amount_cano'].fillna(0)\n",
    "test['amtby_mean_amount_cano'] = test['conam'] / test['mean_amount_cano']\n",
    "test['amtby_mean_amount_cano'] = test['amtby_mean_amount_cano'].fillna(0)\n",
    "\n",
    "median_amount = train.groupby(['bacno'])['conam'].median().rename(\"median_amount\").reset_index()\n",
    "train = train.merge(median_amount)\n",
    "median_amount = test.groupby(['bacno'])['conam'].median().rename(\"median_amount\").reset_index()\n",
    "test = test.merge(median_amount)\n",
    "\n",
    "train['amtby_median_amount'] = train['conam'] / train['median_amount']\n",
    "train['amtby_median_amount'] = train['amtby_median_amount'].fillna(0)\n",
    "test['amtby_median_amount'] = test['conam'] / test['median_amount']\n",
    "test['amtby_median_amount'] = test['amtby_median_amount'].fillna(0)\n",
    "\n",
    "median_amount_cano = train.groupby(['bacno','cano'])['conam'].median().rename(\"median_amount_cano\").reset_index()\n",
    "train = train.merge(median_amount_cano)\n",
    "median_amount_cano = test.groupby(['bacno','cano'])['conam'].median().rename(\"median_amount_cano\").reset_index()\n",
    "test = test.merge(median_amount_cano)\n",
    "\n",
    "train['amtby_median_amount_cano'] = train['conam'] / train['median_amount_cano']\n",
    "train['amtby_median_amount_cano'] = train['amtby_median_amount_cano'].fillna(0)\n",
    "test['amtby_median_amount_cano'] = test['conam'] / test['median_amount_cano']\n",
    "test['amtby_median_amount_cano'] = test['amtby_median_amount_cano'].fillna(0)\n",
    "\n",
    "\n",
    "# std_amount = train.groupby(['bacno'])['conam'].std().rename(\"std_amount\").reset_index()\n",
    "# train = train.merge(std_amount)\n",
    "# std_amount = test.groupby(['bacno'])['conam'].std().rename(\"std_amount\").reset_index()\n",
    "# test = test.merge(std_amount)\n",
    "\n",
    "# std_amount_cano = train.groupby(['bacno','cano'])['conam'].std().rename(\"std_amount_cano\").reset_index()\n",
    "# train = train.merge(std_amount_cano)\n",
    "# std_amount_cano = test.groupby(['bacno','cano'])['conam'].std().rename(\"std_amount_cano\").reset_index()\n",
    "# test = test.merge(std_amount_cano)\n",
    "\n",
    "\n",
    "frequency = ((train.groupby(['bacno'])['locdt'].max()-train.groupby(['bacno'])['locdt'].min())/train.groupby(['bacno'])['locdt'].count()).rename(\"frequency\").reset_index()\n",
    "train = train.merge(frequency)\n",
    "frequency = ((test.groupby(['bacno'])['locdt'].max()-test.groupby(['bacno'])['locdt'].min())/test.groupby(['bacno'])['locdt'].count()).rename(\"frequency\").reset_index()\n",
    "test = test.merge(frequency)\n",
    "\n",
    "frequency_cano = ((train.groupby(['bacno','cano'])['locdt'].max()-train.groupby(['bacno','cano'])['locdt'].min())/train.groupby(['bacno','cano'])['locdt'].count()).rename(\"frequency_cano\").reset_index()\n",
    "train = train.merge(frequency_cano)\n",
    "frequency_cano = ((test.groupby(['bacno','cano'])['locdt'].max()-test.groupby(['bacno','cano'])['locdt'].min())/test.groupby(['bacno','cano'])['locdt'].count()).rename(\"frequency_cano\").reset_index()\n",
    "test = test.merge(frequency_cano)\n",
    "\n",
    "\n",
    "mean_time = train.groupby(['bacno'])['time'].mean().rename(\"mean_time\").reset_index()\n",
    "train = train.merge(mean_time)\n",
    "mean_time = test.groupby(['bacno'])['time'].mean().rename(\"mean_time\").reset_index()\n",
    "test = test.merge(mean_time)\n",
    "\n",
    "median_time = train.groupby(['bacno'])['time'].median().rename(\"median_time\").reset_index()\n",
    "train = train.merge(median_time)\n",
    "median_time = test.groupby(['bacno'])['time'].median().rename(\"median_time\").reset_index()\n",
    "test = test.merge(median_time)\n",
    "\n",
    "\n",
    "mean_time_cano = train.groupby(['bacno','cano'])['time'].mean().rename(\"mean_time_cano\").reset_index()\n",
    "train = train.merge(mean_time_cano)\n",
    "mean_time_cano = test.groupby(['bacno','cano'])['time'].mean().rename(\"mean_time_cano\").reset_index()\n",
    "test = test.merge(mean_time_cano)\n",
    "\n",
    "median_time_cano = train.groupby(['bacno','cano'])['time'].median().rename(\"median_time_cano\").reset_index()\n",
    "train = train.merge(median_time_cano)\n",
    "median_time_cano = test.groupby(['bacno','cano'])['time'].median().rename(\"median_time_cano\").reset_index()\n",
    "test = test.merge(median_time_cano)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sort_values(by = ['total_seconds'])\n",
    "train['total_seconds_diff'] = train.groupby(['bacno'])['total_seconds'].diff()\n",
    "train['total_seconds_diff'] = train['total_seconds_diff'].fillna(0)\n",
    "\n",
    "train = train.sort_values(by = ['total_seconds'])\n",
    "train['total_seconds_diff_cano'] = train.groupby(['bacno','cano'])['total_seconds'].diff()\n",
    "train['total_seconds_diff_cano'] = train['total_seconds_diff_cano'].fillna(0)\n",
    "\n",
    "# train['total_seconds_diff_by_freq'] = train['total_seconds_diff']/train['frequency']\n",
    "# train['total_seconds_diff_by_freq'] = train['total_seconds_diff_by_freq'].fillna(train.total_seconds_diff_by_freq.mean())\n",
    "test = test.sort_values(by = ['total_seconds'])\n",
    "test['total_seconds_diff'] = test.groupby(['bacno'])['total_seconds'].diff()\n",
    "test['total_seconds_diff'] = test['total_seconds_diff'].fillna(0)\n",
    "\n",
    "test = test.sort_values(by = ['total_seconds'])\n",
    "test['total_seconds_diff_cano'] = test.groupby(['bacno','cano'])['total_seconds'].diff()\n",
    "test['total_seconds_diff_cano'] = test['total_seconds_diff_cano'].fillna(0)\n",
    "\n",
    "# test['total_seconds_diff_by_freq'] = test['total_seconds_diff']/test['frequency']\n",
    "# test['total_seconds_diff_by_freq'] = test['total_seconds_diff_by_freq'].fillna(test.total_seconds_diff_by_freq.mean())\n",
    "\n",
    "# train = train.sort_values(by = ['bacno','total_seconds'])\n",
    "# train['conam_change'] = (train.conam!=train.conam.shift()) | (train.bacno!=train.bacno.shift())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train[['fraud_ind']]\n",
    "\n",
    "x_train = train[['acqic', 'bacno', 'cano', 'conam', 'contp', 'csmcu', 'ecfg', 'etymd',\n",
    "       'flbmk', 'flg_3dsmk', 'hcefg', 'insfg', 'iterm','week','hours', 'minutes', 'seconds',\n",
    "       'mcc', 'mchno', 'ovrlt', 'scity', 'stocn', 'stscd',\n",
    "       'decimal', \n",
    "       'transation_count', 'transation_count_cano',\n",
    "       'acqic_duplicated_count', 'conam_duplicated_count', 'conam_stocn',\n",
    "       'bacno_mchno', 'cano_mchno', 'bacno_stocn', 'cano_stocn', 'bacno_scity',\n",
    "       'cano_scity', 'bacno_flg_3dsmk', 'cano_flg_3dsmk', 'bacno_ecfg_mean',\n",
    "       'cano_ecfg_mean', \n",
    "        #'mean_amount',#'mean_amount_cano', #'std_amount', 'std_amount_cano' \n",
    "       'median_amount','amtby_median_amount','amtby_median_amount_cano',\n",
    "       'median_amount_cano' , 'frequency',    \n",
    "       'frequency_cano', 'mean_time', 'median_time', 'mean_time_cano',\n",
    "       'median_time_cano', 'total_seconds_diff', 'total_seconds_diff_cano','amtby_mean_amount','amtby_mean_amount_cano','taiwan_ornot'\n",
    "\n",
    "       ]]\n",
    "\n",
    "testx = test[['acqic', 'bacno', 'cano', 'conam', 'contp', 'csmcu', 'ecfg', 'etymd',\n",
    "       'flbmk', 'flg_3dsmk', 'hcefg', 'insfg', 'iterm','week','hours', 'minutes', 'seconds',\n",
    "       'mcc', 'mchno', 'ovrlt', 'scity', 'stocn', 'stscd',\n",
    "       'decimal', \n",
    "       'transation_count', 'transation_count_cano',\n",
    "       'acqic_duplicated_count', 'conam_duplicated_count', 'conam_stocn',\n",
    "       'bacno_mchno', 'cano_mchno', 'bacno_stocn', 'cano_stocn', 'bacno_scity',\n",
    "       'cano_scity', 'bacno_flg_3dsmk', 'cano_flg_3dsmk', 'bacno_ecfg_mean',\n",
    "       'cano_ecfg_mean', \n",
    "        #'mean_amount',#'mean_amount_cano', #'std_amount', 'std_amount_cano' \n",
    "       'median_amount','amtby_median_amount','amtby_median_amount_cano',\n",
    "       'median_amount_cano' , 'frequency',    \n",
    "       'frequency_cano', 'mean_time', 'median_time', 'mean_time_cano',\n",
    "       'median_time_cano', 'total_seconds_diff', 'total_seconds_diff_cano','amtby_mean_amount','amtby_mean_amount_cano','taiwan_ornot']]\n",
    "\n",
    "# #train_test_split\n",
    "X_train,X_valid,y_train,y_valid = model_selection.train_test_split(x_train, y_train, random_state=12, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import RFECV\n",
    "\n",
    "# # The \"accuracy\" scoring is proportional to the number of correct classifications\n",
    "# clf_rf_4 = lgbm.LGBMClassifier() \n",
    "# rfecv = RFECV(estimator=clf_rf_4, step=1, cv=5,scoring='f1')   #5-fold cross-validation\n",
    "# rfecv = rfecv.fit(X_train, y_train)\n",
    "\n",
    "# print('Optimal number of features :', rfecv.n_features_)\n",
    "# print('Best features :', x_train.columns[rfecv.support_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_val = train[train.locdt>60]\n",
    "# train_train = train[train.locdt<61]\n",
    "\n",
    "# y_valid = train_val[['fraud_ind']]\n",
    "# X_valid = train_val[['acqic', 'bacno', 'cano', 'conam', 'contp', 'csmcu', 'ecfg', 'etymd',\n",
    "#        'flbmk', 'flg_3dsmk', 'hcefg', 'insfg', 'iterm', 'week','total_seconds_diff','transation_count',\n",
    "#        'loctm', 'mcc', 'mchno', 'ovrlt', 'scity', 'stocn', 'stscd'\n",
    "#        ]]\n",
    "\n",
    "# y_train = train_train[['fraud_ind']]\n",
    "# X_train = train_train[['acqic', 'bacno', 'cano', 'conam', 'contp', 'csmcu', 'ecfg', 'etymd',\n",
    "#        'flbmk', 'flg_3dsmk', 'hcefg', 'insfg', 'iterm', 'week','total_seconds_diff','transation_count',\n",
    "#        'loctm', 'mcc', 'mchno', 'ovrlt', 'scity', 'stocn', 'stscd'\n",
    "#        ]]\n",
    "\n",
    "# testx = test[['acqic', 'bacno', 'cano', 'conam', 'contp', 'csmcu', 'ecfg', 'etymd',\n",
    "#        'flbmk', 'flg_3dsmk', 'hcefg', 'insfg', 'iterm', 'week','total_seconds_diff','transation_count',\n",
    "#        'loctm', 'mcc', 'mchno', 'ovrlt', 'scity', 'stocn', 'stscd']]\n",
    "# ##________________________________________________________________________________________________________________\n",
    "# y_train = train[['fraud_ind','locdt','time']]\n",
    "# x_train = train[['acqic', 'bacno', 'cano', 'conam', 'contp', 'csmcu', 'ecfg', 'etymd',\n",
    "#        'flbmk', 'flg_3dsmk', 'hcefg', 'insfg', 'iterm', 'week','total_seconds_diff','transation_count','locdt','time',\n",
    "#        'loctm', 'mcc', 'mchno', 'ovrlt', 'scity', 'stocn', 'stscd']]\n",
    "# testx = test[['acqic', 'bacno', 'cano', 'conam', 'contp', 'csmcu', 'ecfg', 'etymd',\n",
    "#        'flbmk', 'flg_3dsmk', 'hcefg', 'insfg', 'iterm', 'week','total_seconds_diff','transation_count','locdt','time',\n",
    "#        'loctm', 'mcc', 'mchno', 'ovrlt', 'scity', 'stocn', 'stscd']]\n",
    "\n",
    "# X = x_train.sort_values('locdt')\n",
    "# X = X.sort_values('time').drop(['locdt','time'], axis=1)\n",
    "# y = y_train.sort_values('locdt')\n",
    "# y = y.sort_values('time').drop(['locdt','time'], axis=1)\n",
    "# #X_test = test.sort_values('TransactionDT').drop(['TransactionDT', 'TransactionID'], axis=1)\n",
    "# X_test = testx.drop(['locdt','time'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = {\n",
    "      'lgb':lgbm.LGBMClassifier(objective = 'binary', is_unbalance = True, metric = 'binary_logloss,auc', \n",
    "                                max_depth = 35, num_leaves = 1000, learning_rate = 0.01, feature_fraction = 0.6, \n",
    "                                min_child_samples=21, min_child_weight=0.001, \n",
    "                                bagging_fraction = 1, bagging_freq = 2, reg_alpha = 0.001, reg_lambda = 0.3, cat_smooth = 0, num_iterations = 3000  )\n",
    "          \n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def f1_eval(y_pred, dtrain):\n",
    "#     y_true = dtrain.get_label()\n",
    "#     err = 1-f1_score(y_true, np.round(y_pred))\n",
    "#     return 'f1_err', err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_set  = [(X_valid , y_valid)]\n",
    "f1_scores = dict()\n",
    "for clf_name in clfs:\n",
    "    print(clf_name)\n",
    "    clf = clfs[clf_name]\n",
    "#     clf.fit(X_train ,  y_train ,eval_set=eval_set,early_stopping_rounds=50,eval_metric='f1_eval'),#eval_metric='f1_eval'or auc\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_valid)\n",
    "    f1_scores[clf_name] = f1_score(y_pred, y_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_pred, y_valid))\n",
    "print(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tss = TimeSeriesSplit(n_splits=5)\n",
    "# # kfold = KFold(n_splits=5)\n",
    "# scores = cross_val_score(clf, X, y, cv=tss, scoring = 'f1',n_jobs =-1) \n",
    "# print(scores)\n",
    "# print (\"mean validation F1 for Random Forests:\",\n",
    "#        \"%0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat((pd.DataFrame(x_train.columns, columns = ['variable']), \n",
    "           pd.DataFrame(clf.feature_importances_, columns = ['importance'])), \n",
    "          axis = 1).sort_values(by='importance', ascending = False)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['fraud_ind'] =  clf.predict(testx)\n",
    "submit = submit[['txkey']]\n",
    "submit=pd.merge(submit,test,on='txkey') \n",
    "submit['fraud_ind'] = submit['fraud_ind'].astype(int)\n",
    "submit = submit[['txkey','fraud_ind']]\n",
    "submit.to_csv('lgbnootune.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本調參"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "   \n",
    "#     'objective' : 'binary', \n",
    "#     'is_unbalance' : True, \n",
    "#     'metric' : 'binary_logloss,auc',                         \n",
    "#     'max_depth' : 35, \n",
    "#     'num_leaves' : 1000, \n",
    "#     'learning_rate' : 0.01, \n",
    "#     'feature_fraction' : 0.6, \n",
    "#     'min_child_samples':21, \n",
    "#     'min_child_weight':0.001, \n",
    "#     'bagging_fraction' : 1, \n",
    "#     'bagging_freq' : 2, \n",
    "#     'reg_alpha' : 0.001, \n",
    "#     'reg_lambda' : 0.3, \n",
    "#     'cat_smooth' : 0, \n",
    "# #     'num_iterations' : 1000 \n",
    "    \n",
    "\n",
    "#     }\n",
    "\n",
    "# data_train = lgbm.Dataset(X_train,y_train, silent=True)\n",
    "# cv_results = lgbm.cv(\n",
    "#     params, data_train, num_boost_round=10000, nfold=5, stratified=False, shuffle=True, metrics='auc',\n",
    "#     early_stopping_rounds=50, verbose_eval=100, show_stdv=True)#seed\n",
    "\n",
    "# print('best n_estimators:', len(cv_results['auc-mean']))\n",
    "# print('best cv score:', cv_results['auc-mean'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "#     'max_depth': [36,37,38],\n",
    "#     'num_leaves': [1000],\n",
    "    'min_child_samples': [18,19,20,21,22],\n",
    "     'min_child_weight': [0.001,0.002]\n",
    "#     'feature_fraction': [0.6, 0.8, 1],\n",
    "#     'bagging_fraction': [0.8,0.9,1],\n",
    "#      'bagging_freq': [2,3,4],\n",
    "# 'reg_alpha': [0, 0.001, 0.01, 0.03, 0.08, 0.3, 0.5],    \n",
    "# 'reg_lambda': [0, 0.001, 0.01, 0.03, 0.08, 0.3, 0.5],\n",
    "# 'cat_smooth': [0,10,20]\n",
    "}\n",
    "\n",
    "\n",
    "gbm = lgbm.LGBMClassifier(objective = 'binary',\n",
    "                         is_unbalance = True,\n",
    "                         metric = 'binary_logloss,auc',\n",
    "                         max_depth = 37,\n",
    "                         num_leaves = 1000,\n",
    "                         learning_rate = 0.1,\n",
    "                         feature_fraction = 0.7,\n",
    "                         min_child_samples=21,\n",
    "                         min_child_weight=0.001,\n",
    "                         bagging_fraction = 1,\n",
    "                         bagging_freq = 2,\n",
    "                         reg_alpha = 0.001,\n",
    "                         reg_lambda = 8,\n",
    "                         cat_smooth = 0,\n",
    "                         num_iterations = 200,             \n",
    "                         )\n",
    "\n",
    "# eval_set  = [(X_valid , y_valid)]\n",
    "# fit_params={\"early_stopping_rounds\":50, \n",
    "#             \"eval_metric\" : \"auc\", \n",
    "#             \"eval_set\" : [[X_valid , y_valid]]}\n",
    "\n",
    "gsearch = GridSearchCV(gbm, param_grid=parameters, scoring='f1', cv=3)\n",
    "gsearch.fit(X_train,y_train)#**fit_params\n",
    "print('Best params:{0}'.format(gsearch.best_params_))\n",
    "print('bestscore:{0}'.format(gsearch.best_score_))\n",
    "print(gsearch.cv_results_['mean_test_score'])\n",
    "print(gsearch.cv_results_['params'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
